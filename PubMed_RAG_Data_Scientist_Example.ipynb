{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# PubMed Medical Literature Analysis\n",
        "\n",
        "<!-- [PLACEHOLDER: Update these links when notebook is finalized] -->\n",
        "<table style=\"float: left; margin-right: 20px;\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/WandLZhang/pubmed-rag/blob/main/PubMed_RAG_Data_Scientist_Example.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FWandLZhang%2Fpubmed-rag%2Fmain%2FPubMed_RAG_Data_Scientist_Example.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/WandLZhang/pubmed-rag/blob/main/PubMed_RAG_Data_Scientist_Example.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/WandLZhang/pubmed-rag/main/PubMed_RAG_Data_Scientist_Example.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<!-- [PLACEHOLDER: Update share links when notebook is finalized] -->\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/WandLZhang/pubmed-rag/blob/main/PubMed_RAG_Data_Scientist_Example.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/WandLZhang/pubmed-rag/blob/main/PubMed_RAG_Data_Scientist_Example.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/WandLZhang/pubmed-rag/blob/main/PubMed_RAG_Data_Scientist_Example.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/WandLZhang/pubmed-rag/blob/main/PubMed_RAG_Data_Scientist_Example.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/WandLZhang/pubmed-rag/blob/main/PubMed_RAG_Data_Scientist_Example.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49e1e41cea0d"
      },
      "source": [
        "| Authors |\n",
        "| --- |\n",
        "| [Willis Zhang](https://github.com/WandLZhang) |\n",
        "| [Stone Jiang](https://github.com/siduojiang) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "**Blog Post: Medical Literature Analysis with PubMed, BigQuery, Gemini**\n",
        "\n",
        "<a href=\"[blog-post-url-placeholder]\" target=\"_blank\">\n",
        "  <img src=\"https://storage.googleapis.com/[placeholder-image-path]/medical-literature-blog-header.jpg\" alt=\"Medical Literature Analysis with PubMed and Gemini\" width=\"500\">\n",
        "</a>\n",
        "\n",
        "This notebook demonstrates how to analyze medical cases using PubMed literature with BigQuery vector search and Gemini. It converts the basic user experience from the [Capricorn Medical Research Application](https://capricorn-medical-research.web.app/) into an interactive Colab notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d975e698c9a4"
      },
      "source": [
        "\n",
        "In this tutorial, you learn how to:\n",
        "\n",
        "- Extract medical information (disease diagnosis and actionable events) from case notes\n",
        "- Search PubMed literature using BigQuery vector search\n",
        "- Score and rank articles using customizable criteria\n",
        "- Develop evidence-based analysis with citations\n",
        "- Create an interactive chat interface for medical discussions\n",
        "\n",
        "![Medical Literature Analysis Architecture](https://github.com/WandLZhang/pubmed-rag/blob/main/visuals/1.png?raw=true)\n",
        "\n",
        "This tutorial uses the following Google Cloud AI services and resources:\n",
        "\n",
        "- **Vertex AI**: Gemini 2.5 Flash for text analysis and generation\n",
        "- **BigQuery**: Vector search on PubMed article embeddings\n",
        "- **Interactive Widgets**: Customizable scoring configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhpPA8gzBBJt"
      },
      "source": [
        "## Let's begin\n",
        "\n",
        "1. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "2. Install the following packages required to execute this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4ef9b72d43"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet google-genai google-cloud-bigquery google-cloud-bigquery-storage plotly pandas==2.2.2 db-dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX8nMobMBBJu"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "3. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "4. After selecting your project, in the console click the main logo in the top-left to get to your project home.\n",
        "\n",
        "![](https://github.com/WandLZhang/pubmed-rag/blob/main/visuals/2.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wZyL6R1BBJu"
      },
      "source": [
        "This will take you to your project home. Copy the `Project ID` like the above orange box and paste it into the field below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\"}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiBYSuZRBBJu"
      },
      "source": [
        "NOTE: You can change the `LOCATION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations).\n",
        "\n",
        "5. Enable the [Vertex AI APIs](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,discoveryengine.googleapis.com).\n",
        "6. If you are running this notebook on Google Colab, you will need to authenticate your environment. To do this, run the new cell below. This step is not required if you are using Vertex AI Workbench."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "603adbbf0532"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Authenticate user to Google Cloud\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "e336da7161af"
      },
      "source": [
        "## Medical Literature Analysis Pipeline\n",
        "\n",
        "This section implements the complete medical literature analysis workflow, from case notes to treatment recommendations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "6a28ca4abb52"
      },
      "source": [
        "### 1. Initialize Vertex AI and BigQuery Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "hide_input": true,
        "id": "25955ce5d263",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Model Configuration\n",
        "MODEL_ID = \"gemini-2.5-flash\" # @param [\"gemini-2.5-flash\",\"gemini-2.5-pro\"] {\"allow-input\":true, isTemplate: true}\n",
        "THINKING_BUDGET = 0 # @param {type: \"slider\", min: 0, max: 24576, step: 1}\n",
        "\n",
        "# Initialize the Gemini model from Vertex AI:\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "# Initialize BigQuery client\n",
        "from google.cloud import bigquery\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# Configure PubMed dataset (public dataset with embeddings)\n",
        "# [PLACEHOLDER: Update to final public dataset location]\n",
        "PUBMED_DATASET = \"wz-data-catalog-demo.pubmed\"\n",
        "PUBMED_TABLE = f\"{PUBMED_DATASET}.pmid_embed_nonzero_metadata\"  # Combined embeddings and metadata table\n",
        "\n",
        "# User's BigQuery dataset for embedding model\n",
        "USER_DATASET = \"pubmed\"  # @param {type: \"string\"}\n",
        "EMBEDDING_MODEL = f\"{PROJECT_ID}.{USER_DATASET}.textembed\"  # Text embedding model for vector search\n",
        "\n",
        "# Create the dataset if it doesn't exist\n",
        "try:\n",
        "    # Check if dataset exists\n",
        "    dataset_ref = bq_client.dataset(USER_DATASET)\n",
        "    dataset = bq_client.get_dataset(dataset_ref)\n",
        "    print(f\"‚úÖ Dataset '{USER_DATASET}' already exists\")\n",
        "except:\n",
        "    # Create the dataset\n",
        "    dataset = bigquery.Dataset(f\"{PROJECT_ID}.{USER_DATASET}\")\n",
        "    dataset.location = LOCATION\n",
        "    dataset = bq_client.create_dataset(dataset, exists_ok=True)\n",
        "    print(f\"‚úÖ Created dataset '{USER_DATASET}'\")\n",
        "\n",
        "# Journal impact data will be loaded from CSV [PLACEHOLDER]\n",
        "JOURNAL_IMPACT_CSV_URL = \"https://raw.githubusercontent.com/WandLZhang/scimagojr_2024/main/scimagojr_2024.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "5d7cb7cceb99"
      },
      "source": [
        "### 2. Create Text Embedding Model\n",
        "\n",
        "Before running vector searches, we'll create a text embedding model in BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "hide_input": true,
        "id": "check_embedding_model",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Use CREATE MODEL IF NOT EXISTS for simplicity\n",
        "create_model_query = f\"\"\"\n",
        "CREATE MODEL IF NOT EXISTS `{EMBEDDING_MODEL}`\n",
        "  REMOTE WITH CONNECTION DEFAULT\n",
        "  OPTIONS(endpoint='text-embedding-005');\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    query_job = bq_client.query(create_model_query)\n",
        "    query_job.result()  # Wait for the query to complete\n",
        "    print(f\"‚úÖ Successfully created/verified embedding model: {EMBEDDING_MODEL}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to create embedding model: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "u0Z_7bZPBBJv"
      },
      "source": [
        "### 3. Setup Journal Impact Data in BigQuery\n",
        "\n",
        "We'll create a BigQuery table for journal impact data, which helps us score article sources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "hide_input": true,
        "id": "setup_journal_table",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create Journal Impact Table in BigQuery\n",
        "def setup_journal_impact_table():\n",
        "    \"\"\"Create and populate journal impact table if it doesn't exist.\"\"\"\n",
        "    table_id = \"journal_impact\"\n",
        "    table_ref = f\"{PROJECT_ID}.{USER_DATASET}.{table_id}\"\n",
        "\n",
        "    try:\n",
        "        # Check if table exists\n",
        "        try:\n",
        "            table = bq_client.get_table(table_ref)\n",
        "            print(f\"‚úÖ Journal impact table already exists with {table.num_rows} rows\")\n",
        "            return True\n",
        "        except:\n",
        "            # Table doesn't exist, create it\n",
        "            print(f\"üìä Creating journal impact table: {table_ref}\")\n",
        "\n",
        "            # Download and parse CSV\n",
        "            df = pd.read_csv(JOURNAL_IMPACT_CSV_URL, sep=';')\n",
        "\n",
        "            # Convert SJR values from string with commas to float\n",
        "            df['SJR_float'] = df['SJR'].apply(lambda x: float(str(x).replace(',', '')) if pd.notna(x) and str(x) != '' else None)\n",
        "\n",
        "            # Select relevant columns and rename\n",
        "            columns_to_keep = {\n",
        "                'Title': 'journal_title',\n",
        "                'SJR_float': 'sjr',\n",
        "                'Issn': 'issn',\n",
        "                'SJR Best Quartile': 'sjr_best_quartile',\n",
        "                'H index': 'h_index',\n",
        "                'Publisher': 'publisher',\n",
        "                'Categories': 'categories',\n",
        "                'Country': 'country',\n",
        "                'Type': 'type'\n",
        "            }\n",
        "\n",
        "            df_clean = df[list(columns_to_keep.keys())].rename(columns=columns_to_keep)\n",
        "\n",
        "            # Remove rows with no SJR value\n",
        "            df_clean = df_clean[df_clean['sjr'].notna()]\n",
        "\n",
        "            print(f\"üìà Cleaned data: {len(df_clean)} rows with valid SJR values\")\n",
        "\n",
        "            # Define table schema\n",
        "            schema = [\n",
        "                bigquery.SchemaField(\"journal_title\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"sjr\", \"FLOAT64\"),\n",
        "                bigquery.SchemaField(\"issn\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"sjr_best_quartile\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"h_index\", \"INT64\"),\n",
        "                bigquery.SchemaField(\"publisher\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"categories\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"country\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"type\", \"STRING\"),\n",
        "            ]\n",
        "\n",
        "            # Configure load job\n",
        "            job_config = bigquery.LoadJobConfig(\n",
        "                schema=schema,\n",
        "                write_disposition=\"WRITE_TRUNCATE\",\n",
        "            )\n",
        "\n",
        "            # Load data\n",
        "            print(f\"‚¨ÜÔ∏è Uploading {len(df_clean)} rows to {table_ref}...\")\n",
        "            job = bq_client.load_table_from_dataframe(df_clean, table_ref, job_config=job_config)\n",
        "            job.result()  # Wait for job to complete\n",
        "\n",
        "            # Verify upload\n",
        "            table = bq_client.get_table(table_ref)\n",
        "            print(f\"‚úÖ Successfully created journal impact table with {table.num_rows} rows\")\n",
        "            return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error setting up journal impact table: {e}\")\n",
        "        # Continue without journal impact data\n",
        "        return False\n",
        "\n",
        "# Setup the journal impact table\n",
        "setup_journal_impact_table()\n",
        "\n",
        "# Load journal data from BigQuery for local lookups\n",
        "def load_journal_data_from_bigquery():\n",
        "    \"\"\"Load journal data from BigQuery table.\"\"\"\n",
        "    try:\n",
        "        query = f\"\"\"\n",
        "        SELECT\n",
        "            journal_title,\n",
        "            sjr\n",
        "        FROM `{PROJECT_ID}.{USER_DATASET}.journal_impact`\n",
        "        WHERE sjr IS NOT NULL\n",
        "        ORDER BY sjr DESC\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"üì• Loading journal data from BigQuery...\")\n",
        "        results = bq_client.query(query).to_dataframe()\n",
        "\n",
        "        # Convert to dictionary\n",
        "        journal_dict = dict(zip(results['journal_title'], results['sjr']))\n",
        "        print(f\"‚úÖ Loaded {len(journal_dict)} journals from BigQuery\")\n",
        "        return journal_dict\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading journal data from BigQuery: {e}\")\n",
        "        return {}\n",
        "\n",
        "# Load the journal impact dictionary\n",
        "journal_impact_dict = load_journal_data_from_bigquery()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "59c98ab0f5fb"
      },
      "source": [
        "### 4. Customizable Scoring System\n",
        "\n",
        "Configure how articles are scored based on various factors. Adjust the sliders to match your research priorities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "hide_input": true,
        "id": "5ebdda19afad",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Dynamic Scoring Configuration System\n",
        "\n",
        "class DynamicScoringConfig:\n",
        "    \"\"\"Configuration class for dynamic scoring criteria.\"\"\"\n",
        "\n",
        "    def __init__(self, criteria_list):\n",
        "        \"\"\"\n",
        "        Initialize with a list of criteria dictionaries.\n",
        "        Each criterion should have: name, description, type, weight\n",
        "        \"\"\"\n",
        "        self.criteria_list = criteria_list\n",
        "        self.config = {c['name']: c['weight'] for c in criteria_list}\n",
        "        self.categories = {c['name']: {'description': c['description']} for c in criteria_list}\n",
        "        self.criteria_by_name = {c['name']: c for c in criteria_list}\n",
        "\n",
        "        # Separate special criteria that need custom handling\n",
        "        self.special_criteria = ['journal_impact', 'year_penalty', 'event_match']\n",
        "\n",
        "        # For compatibility with existing code\n",
        "        self.default_categories = list(self.config.keys())\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Return current configuration as dictionary.\"\"\"\n",
        "        return self.config\n",
        "\n",
        "    def get_criteria(self):\n",
        "        \"\"\"Get criteria for analysis (excluding special ones).\"\"\"\n",
        "        return [c for c in self.criteria_list if c['type'] != 'special']\n",
        "\n",
        "    def get_all_criteria(self):\n",
        "        \"\"\"Get all criteria including special ones.\"\"\"\n",
        "        return self.criteria_list\n",
        "\n",
        "    def get_criterion(self, name):\n",
        "        \"\"\"Get a specific criterion by name.\"\"\"\n",
        "        return self.criteria_by_name.get(name, {})\n",
        "\n",
        "    def get_category_types(self):\n",
        "        \"\"\"Return empty dict - no hardcoded category types.\"\"\"\n",
        "        # This method exists for compatibility but returns empty\n",
        "        # since we're not using category types anymore\n",
        "        return {}\n",
        "\n",
        "# Note: The actual scoring_config will be initialized later in Step 1 with user-defined criteria\n",
        "print(\"‚úÖ Dynamic scoring system loaded. Criteria will be defined in Step 1: Run the Analysis Pipeline.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "persona_config"
      },
      "source": [
        "### 5. Analysis Persona Configuration\n",
        "\n",
        "Customize your research perspective to tailor how articles are analyzed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "hide_input": true,
        "id": "persona_widget",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Analysis Persona Configuration\n",
        "ANALYSIS_PERSONA = \"You are a medical researcher analyzing literature for clinical relevance and treatment insights.\" # @param {type: \"string\"}\n",
        "\n",
        "# Simple persona configuration class for compatibility\n",
        "class SimplePersonaConfig:\n",
        "    def __init__(self, persona):\n",
        "        self.persona = persona\n",
        "\n",
        "    def get_persona(self):\n",
        "        return self.persona\n",
        "\n",
        "# Initialize persona configuration\n",
        "persona_config = SimplePersonaConfig(ANALYSIS_PERSONA)\n",
        "print(\"‚úÖ Analysis persona configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "a29c93ef3f34"
      },
      "source": [
        "### 6. Medical Information Extraction Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "hide_input": true,
        "id": "b2587492ab3f",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "from google.genai.types import GenerateContentConfig\n",
        "\n",
        "# Default extraction prompts\n",
        "DISEASE_EXTRACTION_PROMPT = \"\"\"You are an expert pediatric oncologist analyzing patient case notes to identify the primary disease.\n",
        "\n",
        "Task: Extract the initial diagnosis exactly as written in the case notes.\n",
        "\n",
        "Examples:\n",
        "- Input: \"A now almost 4-year-old female diagnosed with KMT2A-rearranged AML and CNS2 involvement...\"\n",
        "  Output: AML\n",
        "\n",
        "- Input: \"18 y/o boy, diagnosed in November 2021 with T-ALL with CNS1...\"\n",
        "  Output: T-ALL\n",
        "\n",
        "- Input: \"A 10-year-old patient with relapsed B-cell acute lymphoblastic leukemia (B-ALL)...\"\n",
        "  Output: B-cell acute lymphoblastic leukemia (B-ALL)\n",
        "\n",
        "Output only the disease name. No additional text or formatting.\"\"\"\n",
        "\n",
        "EVENT_EXTRACTION_PROMPT = \"\"\"You are an expert pediatric oncologist analyzing patient case notes to identify key disease concepts and clinical features for literature search.\n",
        "\n",
        "Task: Extract 5 general medical concepts that would help find relevant literature. Focus on:\n",
        "- Disease types and subtypes (e.g., \"AML\", \"T-ALL\", \"B-ALL\")\n",
        "- Genetic alterations (gene names only, e.g., \"KMT2A rearrangement\", \"FLT3 mutation\", \"TP53 mutation\")\n",
        "- Treatment modalities (e.g., \"HSCT\", \"chemotherapy\", \"CAR-T therapy\", \"stem cell transplant\")\n",
        "- General complications (e.g., \"relapse\", \"refractory disease\", \"CNS involvement\", \"MRD positive\")\n",
        "- Anatomical sites or disease features (e.g., \"bone marrow\", \"extramedullary disease\")\n",
        "\n",
        "Instructions:\n",
        "- Extract GENERAL CONCEPTS that appear in medical literature\n",
        "- DO NOT include patient-specific details like percentages, timeframes, or specific protocol names\n",
        "- Focus on searchable medical terms\n",
        "- Output exactly 5 concepts\n",
        "\n",
        "Example:\n",
        "Input: \"A 4-year-old female with KMT2A-rearranged AML and CNS2 involvement exhibited refractory disease after NOPHO protocol. MRD remained at 35%. She relapsed 10 months after cord blood HSCT with 33% blasts. WES showed KMT2A::MLLT3 fusion and NRAS mutation.\"\n",
        "\n",
        "Output: \"AML\" \"KMT2A rearrangement\" \"CNS involvement\" \"refractory disease\" \"HSCT relapse\"\n",
        "\n",
        "Output only 5 general medical concepts, one per line in quotes. No additional text or formatting.\"\"\"\n",
        "\n",
        "def extract_medical_info(case_text, info_type=\"both\", disease_prompt=None, events_prompt=None):\n",
        "    \"\"\"Extract disease and actionable events from case notes with customizable prompts.\"\"\"\n",
        "\n",
        "    # Use provided prompts or defaults\n",
        "    prompts = {\n",
        "        \"disease\": disease_prompt or DISEASE_EXTRACTION_PROMPT,\n",
        "        \"events\": events_prompt or EVENT_EXTRACTION_PROMPT\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for key, prompt in prompts.items():\n",
        "        if info_type == \"both\" or info_type == key:\n",
        "            full_prompt = f\"{prompt}\\n\\nCase notes:\\n{case_text}\"\n",
        "\n",
        "            # Match the original implementation - no max_output_tokens specified\n",
        "            response = client.models.generate_content(\n",
        "                model=MODEL_ID,\n",
        "                contents=[full_prompt],\n",
        "                config=GenerateContentConfig(\n",
        "                    temperature=0,\n",
        "                    thinking_config=types.ThinkingConfig(thinking_budget=THINKING_BUDGET)\n",
        "                )\n",
        "            )\n",
        "\n",
        "            results[key] = response.text.strip()\n",
        "\n",
        "    # Process events to create ID mapping\n",
        "    if 'events' in results:\n",
        "        events_text = results['events']\n",
        "        events_list = []\n",
        "        events_with_ids = {}\n",
        "\n",
        "        # Parse events (handle both line-separated and quote-separated formats)\n",
        "        if '\"' in events_text:\n",
        "            # Events are in quotes\n",
        "            import re\n",
        "            events_list = re.findall(r'\"([^\"]+)\"', events_text)\n",
        "        else:\n",
        "            # Events are line-separated or comma-separated\n",
        "            events_list = [e.strip() for e in events_text.replace('\\n', ',').split(',') if e.strip()]\n",
        "\n",
        "        # Create ID mapping\n",
        "        for i, event in enumerate(events_list, 1):\n",
        "            event_id = f\"event_{i}\"\n",
        "            events_with_ids[event_id] = event\n",
        "\n",
        "        results['events_list'] = events_list\n",
        "        results['events_with_ids'] = events_with_ids\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "6708e03a1d7b"
      },
      "source": [
        "### 7. Article Scoring Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "hide_input": true,
        "id": "0a9ce21ca573",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from datetime import datetime\n",
        "\n",
        "def normalize_journal_score(sjr, max_points):\n",
        "    \"\"\"Normalize journal SJR score to specified max points.\"\"\"\n",
        "    if not sjr or sjr <= 0:\n",
        "        return 0\n",
        "\n",
        "    # Use log scale to handle large range of SJR values\n",
        "    # Typical SJR ranges from 0 to ~100,000\n",
        "    normalized = math.log(sjr + 1) * (max_points / 12)  # log(100000) ‚âà 11.5\n",
        "    return min(normalized, max_points)\n",
        "\n",
        "def calculate_article_score(metadata, config, query_disease=None):\n",
        "    \"\"\"Calculate article score based on metadata and dynamic user configuration.\"\"\"\n",
        "    score = 0\n",
        "    breakdown = {}\n",
        "\n",
        "    # Get the scoring config instance to access criteria details\n",
        "    if hasattr(config, 'get_all_criteria'):\n",
        "        # If config is a DynamicScoringConfig instance\n",
        "        all_criteria = config.get_all_criteria()\n",
        "        weights = config.get_config()\n",
        "    else:\n",
        "        # If config is just a dictionary (for backwards compatibility)\n",
        "        weights = config\n",
        "        all_criteria = [{'name': k, 'type': 'boolean', 'weight': v} for k, v in weights.items()]\n",
        "\n",
        "    # Process each criterion\n",
        "    for criterion in all_criteria:\n",
        "        name = criterion['name']\n",
        "        weight = criterion['weight']\n",
        "        criterion_type = criterion.get('type', 'boolean')\n",
        "\n",
        "        # Special handling for specific criteria that need computation\n",
        "        if name == 'journal_impact' and metadata.get('journal_sjr'):\n",
        "            sjr = float(metadata['journal_sjr'])\n",
        "            if sjr > 0:\n",
        "                impact_points = normalize_journal_score(sjr, weight)\n",
        "                score += impact_points\n",
        "                breakdown['journal_impact'] = round(impact_points, 2)\n",
        "\n",
        "        elif name == 'year_penalty' and metadata.get('year'):\n",
        "            try:\n",
        "                current_year = datetime.now().year\n",
        "                article_year = int(metadata['year'])\n",
        "                year_diff = current_year - article_year\n",
        "                year_points = weight * year_diff\n",
        "                score += year_points\n",
        "                breakdown['year'] = year_points\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        elif name == 'event_match':\n",
        "            # Handle event matching\n",
        "            events = metadata.get('actionable_events', [])\n",
        "            if events is None:\n",
        "                events = []\n",
        "\n",
        "            # Handle different formats of actionable_events\n",
        "            if isinstance(events, str):\n",
        "                # If it's a comma-separated string\n",
        "                events = [e.strip() for e in events.split(',') if e.strip()]\n",
        "                matched_events = len(events)\n",
        "            elif isinstance(events, list):\n",
        "                # If it's a list of dicts with 'matches_query' field\n",
        "                matched_events = sum(1 for event in events if isinstance(event, dict) and event.get('matches_query', False))\n",
        "            else:\n",
        "                matched_events = 0\n",
        "\n",
        "            if matched_events > 0:\n",
        "                event_points = matched_events * weight\n",
        "                score += event_points\n",
        "                breakdown['event_match'] = event_points\n",
        "\n",
        "        # Generic handling for all other criteria\n",
        "        elif criterion_type == 'boolean' and metadata.get(name):\n",
        "            score += weight\n",
        "            breakdown[name] = weight\n",
        "\n",
        "        elif criterion_type == 'numeric':\n",
        "            value = metadata.get(name, 0)\n",
        "            if value:\n",
        "                # Assume numeric values are 0-100 scale\n",
        "                points = weight * (value / 100)\n",
        "                score += points\n",
        "                breakdown[name] = round(points, 2)\n",
        "\n",
        "        elif criterion_type == 'direct':\n",
        "            value = metadata.get(name, 0)\n",
        "            if value:\n",
        "                # Direct multiplication\n",
        "                points = weight * value\n",
        "                score += points\n",
        "                breakdown[name] = round(points, 2)\n",
        "\n",
        "    return round(score, 2), breakdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "77f0800f8762"
      },
      "source": [
        "### 8. BigQuery Vector Search Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "hide_input": true,
        "id": "vector_search_func",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "def generate_embedding(text):\n",
        "    \"\"\"Generate text embedding using Gemini.\"\"\"\n",
        "    from vertexai.language_models import TextEmbeddingModel\n",
        "\n",
        "    model = TextEmbeddingModel.from_pretrained(\"text-embedding-005\")\n",
        "    embeddings = model.get_embeddings([text])\n",
        "    return embeddings[0].values\n",
        "\n",
        "def search_pubmed_articles(disease, events_list, top_k=15, offset=0):\n",
        "    \"\"\"Search PubMed articles using BigQuery vector similarity.\"\"\"\n",
        "\n",
        "    # Combine disease and events for search query\n",
        "    query_text = f\"{disease} {' '.join(events_list)}\"\n",
        "\n",
        "    # Create the SQL query with offset support\n",
        "    sql = f\"\"\"\n",
        "    DECLARE query_text STRING;\n",
        "    SET query_text = \\\"\\\"\\\"\n",
        "{query_text}\n",
        "\\\"\\\"\\\";\n",
        "\n",
        "    WITH vector_results AS (\n",
        "        SELECT base.name AS PMCID, base.PMID, base.content, distance\n",
        "        FROM VECTOR_SEARCH(\n",
        "            TABLE `{PUBMED_TABLE}`,\n",
        "            'ml_generate_embedding_result',\n",
        "            (SELECT ml_generate_embedding_result\n",
        "             FROM ML.GENERATE_EMBEDDING(\n",
        "                 MODEL `{EMBEDDING_MODEL}`,\n",
        "                 (SELECT query_text AS content)\n",
        "             )),\n",
        "            top_k => {top_k + offset}\n",
        "        )\n",
        "    )\n",
        "    SELECT * FROM vector_results\n",
        "    ORDER BY distance\n",
        "    LIMIT {top_k}\n",
        "    OFFSET {offset}\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute query\n",
        "    results = bq_client.query(sql).to_dataframe()\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "article_analysis"
      },
      "source": [
        "### 9. Article Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "hide_input": true,
        "id": "analyze_articles",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "\n",
        "def analyze_article_batch(articles_df, disease, events_list, scoring_config):\n",
        "    \"\"\"Analyze a batch of articles using Gemini with dynamic criteria.\"\"\"\n",
        "\n",
        "    # Build journal context for Gemini to look up journal titles\n",
        "    journal_context = \"\"\n",
        "    for title, sjr in journal_impact_dict.items():\n",
        "        journal_context += f\"- {title}: {sjr}\\n\"\n",
        "\n",
        "    # Get all criteria from scoring config\n",
        "    criteria = scoring_config.get_criteria()\n",
        "\n",
        "    # Build analysis prompt with dynamic criteria\n",
        "    criteria_prompts = []\n",
        "    field_counter = 1\n",
        "\n",
        "    # Add standard fields first\n",
        "    criteria_prompts.append(f\"{field_counter}. disease_match: Does the article discuss {disease}? (true/false)\")\n",
        "    field_counter += 1\n",
        "    criteria_prompts.append(f\"{field_counter}. title: Article title\")\n",
        "    field_counter += 1\n",
        "    criteria_prompts.append(f\"{field_counter}. journal_title: Extract the journal name from the article and match it to the list above\")\n",
        "    field_counter += 1\n",
        "    criteria_prompts.append(f\"{field_counter}. journal_sjr: Use the SJR score from the matched journal (0 if not found)\")\n",
        "    field_counter += 1\n",
        "    criteria_prompts.append(f\"{field_counter}. year: Publication year\")\n",
        "    field_counter += 1\n",
        "    criteria_prompts.append(f\"{field_counter}. actionable_events: List which of these events are mentioned: {events_list}\")\n",
        "    field_counter += 1\n",
        "    criteria_prompts.append(f\"{field_counter}. paper_type: Type of study (Clinical Trial, Review, Case Report, etc.)\")\n",
        "    field_counter += 1\n",
        "    criteria_prompts.append(f\"{field_counter}. key_findings: Brief summary of main findings (1-2 sentences)\")\n",
        "    field_counter += 1\n",
        "\n",
        "    # Add dynamic criteria\n",
        "    for criterion in criteria:\n",
        "        if criterion['type'] == 'boolean':\n",
        "            criteria_prompts.append(f\"{field_counter}. {criterion['name']}: {criterion['description']} (true/false)\")\n",
        "        elif criterion['type'] == 'numeric':\n",
        "            criteria_prompts.append(f\"{field_counter}. {criterion['name']}: {criterion['description']} (0-100 scale)\")\n",
        "        elif criterion['type'] == 'direct':\n",
        "            criteria_prompts.append(f\"{field_counter}. {criterion['name']}: {criterion['description']} (count)\")\n",
        "        field_counter += 1\n",
        "\n",
        "    criteria_text = \"\\n    \".join(criteria_prompts)\n",
        "\n",
        "    prompt = f\"\"\"Analyze these medical research articles for relevance to:\n",
        "    Disease: {disease}\n",
        "    Actionable Events: {', '.join(events_list)}\n",
        "\n",
        "    IMPORTANT: When extracting journal information, use the following journal impact data to find the matching journal title and its SJR score:\n",
        "\n",
        "    Journal Impact Data (SJR scores):\n",
        "{journal_context}\n",
        "\n",
        "    For each article, extract:\n",
        "    {criteria_text}\n",
        "\n",
        "    Return as JSON array with one object per article.\n",
        "\n",
        "    Articles:\n",
        "    \"\"\"\n",
        "\n",
        "    # Format articles for analysis - use FULL CONTENT\n",
        "    articles_text = \"\"\n",
        "    for _, article in articles_df.iterrows():\n",
        "        content = article.get('content', article.get('abstract', ''))\n",
        "        articles_text += f\"\"\"\\n---\\nPMID: {article['PMID']}\n",
        "Article Content: {content[:3000]}...\\n\"\"\"  # Limit content for token management\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=[prompt + articles_text],\n",
        "        config=GenerateContentConfig(\n",
        "            temperature=0,\n",
        "            response_mime_type=\"application/json\",\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        return json.loads(response.text)\n",
        "    except:\n",
        "        print(\"Failed to parse response:\", response.text)\n",
        "        return []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "1b308548c68b"
      },
      "source": [
        "### 10. Complete Medical Analysis Pipeline\n",
        "\n",
        "Now let's put it all together to analyze medical cases with the two-phase approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "hide_input": true,
        "id": "complete_pipeline",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "def process_medical_case(case_text,\n",
        "                        default_articles=5,     # Articles per batch\n",
        "                        min_per_event=3,        # Minimum articles per event\n",
        "                        max_articles=50):       # Maximum total to search\n",
        "    \"\"\"\n",
        "    Complete pipeline to process medical case notes with two-phase analysis.\n",
        "\n",
        "    Parameters:\n",
        "    - case_text: The medical case description\n",
        "    - default_articles: Number of articles to retrieve per batch (default: 5)\n",
        "    - min_per_event: Minimum articles required per actionable event (default: 3)\n",
        "    - max_articles: Maximum total articles to search (default: 50)\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üî¨ Extracting medical information...\")\n",
        "    # Extract disease and events\n",
        "    medical_info = extract_medical_info(case_text)\n",
        "    disease = medical_info.get('disease', '')\n",
        "    events_with_ids = medical_info.get('events_with_ids', {})\n",
        "    events_list = medical_info.get('events_list', [])\n",
        "\n",
        "    print(f\"\\nüìã Disease: {disease}\")\n",
        "    print(f\"üß¨ Actionable Events: {', '.join(events_list)}\")\n",
        "\n",
        "    # Phase 1: Progressive search with event coverage tracking\n",
        "    print(f\"\\nüîç Phase 1: Searching for articles with event coverage...\")\n",
        "    print(f\"   Target: {min_per_event} articles per event\")\n",
        "\n",
        "    event_coverage = {event_id: [] for event_id in events_with_ids.keys()}\n",
        "    total_articles_searched = 0\n",
        "    all_articles = []\n",
        "\n",
        "    while total_articles_searched < max_articles:\n",
        "        # Check if all events have minimum coverage\n",
        "        all_covered = all(len(pmcids) >= min_per_event for pmcids in event_coverage.values())\n",
        "        if all_covered:\n",
        "            print(f\"‚úÖ All events have minimum coverage!\")\n",
        "            break\n",
        "\n",
        "        # Search next batch\n",
        "        print(f\"\\n   Searching articles {total_articles_searched + 1}-{total_articles_searched + default_articles}...\")\n",
        "\n",
        "        articles_df = search_pubmed_articles(\n",
        "            disease, events_list,\n",
        "            top_k=default_articles,\n",
        "            offset=total_articles_searched\n",
        "        )\n",
        "\n",
        "        if articles_df.empty:\n",
        "            print(\"   No more articles found.\")\n",
        "            break\n",
        "\n",
        "        all_articles.append(articles_df)\n",
        "\n",
        "        # Quick event coverage check (simplified version)\n",
        "        # In the full app, this uses AI.GENERATE_TABLE for batch processing\n",
        "        for idx, row in articles_df.iterrows():\n",
        "            content = row.get('content', '')\n",
        "            pmcid = row.get('PMCID')\n",
        "\n",
        "            # Check which events are mentioned\n",
        "            for event_id, event_text in events_with_ids.items():\n",
        "                if event_text.lower() in content.lower():\n",
        "                    if pmcid not in event_coverage[event_id]:\n",
        "                        event_coverage[event_id].append(pmcid)\n",
        "\n",
        "        total_articles_searched += len(articles_df)\n",
        "\n",
        "        # Report coverage\n",
        "        print(\"\\n   Event coverage status:\")\n",
        "        for event_id, event_text in events_with_ids.items():\n",
        "            count = len(event_coverage[event_id])\n",
        "            status = \"‚úì\" if count >= min_per_event else \" \"\n",
        "            print(f\"   {status} {event_text}: {count}/{min_per_event}\")\n",
        "\n",
        "    # Combine all articles\n",
        "    if not all_articles:\n",
        "        print(\"‚ùå No articles found\")\n",
        "        return {\n",
        "            'disease': disease,\n",
        "            'events': events_list,\n",
        "            'articles': pd.DataFrame(),\n",
        "            'case_text': case_text\n",
        "        }\n",
        "\n",
        "    articles_df = pd.concat(all_articles, ignore_index=True)\n",
        "    print(f\"\\nüìä Phase 2: Analyzing {len(articles_df)} articles...\")\n",
        "\n",
        "    # Analyze articles ONE BY ONE for better feedback\n",
        "    all_analyses = []\n",
        "\n",
        "    print(\"\\nüîÑ Starting detailed analysis...\")\n",
        "    for idx, (_, article_row) in enumerate(articles_df.iterrows()):\n",
        "        pmid = article_row.get('PMID', 'N/A')\n",
        "        pmcid = article_row.get('PMCID', 'N/A')\n",
        "\n",
        "        print(f\"\\nüìÑ Analyzing article {idx + 1}/{len(articles_df)}: PMID {pmid}\")\n",
        "\n",
        "        # Create single-article DataFrame\n",
        "        single_article_df = pd.DataFrame([article_row])\n",
        "\n",
        "        # Analyze this one article\n",
        "        try:\n",
        "            analysis_result = analyze_article_batch(single_article_df, disease, events_list, scoring_config)\n",
        "            if analysis_result and len(analysis_result) > 0:\n",
        "                analysis = analysis_result[0]\n",
        "                all_analyses.append(analysis)\n",
        "\n",
        "                # Show immediate feedback\n",
        "                title = analysis.get('title', 'Unknown title')\n",
        "                if len(title) > 70:\n",
        "                    title = title[:67] + \"...\"\n",
        "                print(f\"   ‚úÖ Title: {title}\")\n",
        "                print(f\"   üìö Journal: {analysis.get('journal_title', 'Unknown')}\")\n",
        "                print(f\"   üìÖ Year: {analysis.get('year', 'N/A')}\")\n",
        "\n",
        "                # Show which events were found\n",
        "                events_found = analysis.get('actionable_events', [])\n",
        "                if events_found:\n",
        "                    print(f\"   üéØ Events found: {', '.join(events_found)}\")\n",
        "            else:\n",
        "                print(f\"   ‚ö†Ô∏è No analysis results returned\")\n",
        "                all_analyses.append({})\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error analyzing article: {str(e)}\")\n",
        "            all_analyses.append({})\n",
        "\n",
        "    # Merge analysis with article data\n",
        "    print(\"\\nüìù Processing analysis results...\")\n",
        "    for idx, analysis in enumerate(all_analyses):\n",
        "        if idx < len(articles_df) and analysis:\n",
        "            for key, value in analysis.items():\n",
        "                # Ensure the column exists before setting values\n",
        "                if key not in articles_df.columns:\n",
        "                    articles_df[key] = None\n",
        "\n",
        "                if key == 'actionable_events':\n",
        "                    # Mark which events match the query\n",
        "                    matched_events = []\n",
        "                    for event in value:\n",
        "                        matched = any(qe.lower() in event.lower() for qe in events_list)\n",
        "                        matched_events.append({\n",
        "                            'event': event,\n",
        "                            'matches_query': matched\n",
        "                        })\n",
        "                    articles_df.at[articles_df.index[idx], key] = matched_events\n",
        "                else:\n",
        "                    articles_df.at[articles_df.index[idx], key] = value\n",
        "\n",
        "    # Calculate scores\n",
        "    print(\"\\nüéØ Calculating scores...\")\n",
        "    config = scoring_config.get_config()\n",
        "    scores = []\n",
        "    breakdowns = []\n",
        "\n",
        "    for _, article in articles_df.iterrows():\n",
        "        metadata = article.to_dict()\n",
        "        score, breakdown = calculate_article_score(metadata, config, disease)\n",
        "        scores.append(score)\n",
        "        breakdowns.append(breakdown)\n",
        "\n",
        "    articles_df['score'] = scores\n",
        "    articles_df['score_breakdown'] = breakdowns\n",
        "\n",
        "    # Sort by score\n",
        "    articles_df = articles_df.sort_values('score', ascending=False)\n",
        "\n",
        "    # Show final summary with top articles\n",
        "    print(f\"\\n‚úÖ Analysis complete! Found {len(articles_df)} articles.\")\n",
        "    print(f\"\\nüèÜ Top 3 articles by score:\")\n",
        "    for idx, (_, article) in enumerate(articles_df.head(3).iterrows()):\n",
        "        title = article.get('title', 'Unknown')\n",
        "        if len(title) > 60:\n",
        "            title = title[:57] + \"...\"\n",
        "        print(f\"   {idx + 1}. Score {article['score']:.1f}: {title}\")\n",
        "\n",
        "    return {\n",
        "        'disease': disease,\n",
        "        'events': events_list,\n",
        "        'articles': articles_df,\n",
        "        'case_text': case_text,\n",
        "        'event_coverage': event_coverage,\n",
        "        'total_searched': total_articles_searched\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "fcd767476241"
      },
      "source": [
        "### 11. Results Visualization Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "hide_input": true,
        "id": "Q3yp1SnbBBJ6",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from IPython.display import display, HTML, Markdown\n",
        "\n",
        "def visualize_results(results):\n",
        "    \"\"\"Create visualizations for the analysis results.\"\"\"\n",
        "    articles_df = results['articles']\n",
        "\n",
        "    # Score distribution chart\n",
        "    fig_scores = px.bar(\n",
        "        articles_df.head(10),\n",
        "        x='PMID',\n",
        "        y='score',\n",
        "        title='Top 10 Articles by Score',\n",
        "        labels={'PMID': 'PMID', 'score': 'Score'},\n",
        "        color='score',\n",
        "        color_continuous_scale='viridis'\n",
        "    )\n",
        "    fig_scores.update_layout(xaxis_tickangle=-45)\n",
        "    fig_scores.show()\n",
        "\n",
        "    # Score breakdown for top article\n",
        "    if len(articles_df) > 0:\n",
        "        top_article = articles_df.iloc[0]\n",
        "        breakdown = top_article['score_breakdown']\n",
        "\n",
        "        fig_breakdown = go.Figure(data=[\n",
        "            go.Bar(\n",
        "                x=list(breakdown.keys()),\n",
        "                y=list(breakdown.values()),\n",
        "                marker_color=['green' if v > 0 else 'red' for v in breakdown.values()]\n",
        "            )\n",
        "        ])\n",
        "        fig_breakdown.update_layout(\n",
        "            title=f\"Score Breakdown for Top Article (PMID: {top_article['PMID']})\",\n",
        "            xaxis_title=\"Scoring Factor\",\n",
        "            yaxis_title=\"Points\"\n",
        "        )\n",
        "        fig_breakdown.show()\n",
        "\n",
        "\n",
        "def display_top_articles(results, n=5):\n",
        "    \"\"\"Display detailed information about top articles.\"\"\"\n",
        "    articles_df = results['articles']\n",
        "\n",
        "    for idx, (_, article) in enumerate(articles_df.head(n).iterrows()):\n",
        "        # Get journal name - handle both possible field names\n",
        "        journal = article.get('journal_title', article.get('journal', 'Unknown'))\n",
        "\n",
        "        display(HTML(f\"\"\"\n",
        "        <div style=\"border: 2px solid #ddd; padding: 15px; margin: 10px 0; border-radius: 5px;\">\n",
        "            <h3>#{idx + 1} - Score: {article['score']:.1f}</h3>\n",
        "            <p><strong>Title:</strong> {article.get('title', 'N/A')}</p>\n",
        "            <p><strong>PMID:</strong> <a href=\"https://pubmed.ncbi.nlm.nih.gov/{article['PMID']}\" target=\"_blank\">{article['PMID']}</a></p>\n",
        "            <p><strong>Journal:</strong> {journal} ({article.get('year', 'N/A')})</p>\n",
        "            <p><strong>Key Findings:</strong> {article.get('key_findings', 'N/A')}</p>\n",
        "            <details>\n",
        "                <summary>Score Breakdown</summary>\n",
        "                <ul>\n",
        "                    {''.join([f\"<li>{k}: {v}</li>\" for k, v in article['score_breakdown'].items()])}\n",
        "                </ul>\n",
        "            </details>\n",
        "        </div>\n",
        "        \"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "final_analysis_section"
      },
      "source": [
        "### 12. Final Analysis\n",
        "\n",
        "Generate comprehensive literature synthesis with customizable analysis prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "hide_input": true,
        "id": "final_analysis_functions",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Final Analysis Functions\n",
        "\n",
        "def format_article_for_analysis(article, idx):\n",
        "    \"\"\"Format a single article for the analysis prompt.\"\"\"\n",
        "    metadata = article.get('metadata', article)\n",
        "\n",
        "    # Get events found\n",
        "    events_found = metadata.get('actionable_events', 'None')\n",
        "    if isinstance(events_found, str) and events_found:\n",
        "        events_str = events_found\n",
        "    else:\n",
        "        events_str = \"None identified\"\n",
        "\n",
        "    # Handle journal info - try different fields\n",
        "    journal = metadata.get('journal_title', metadata.get('journal', 'Unknown'))\n",
        "\n",
        "    # Fix: Check for both uppercase and lowercase field names\n",
        "    pmid = article.get('PMID') or article.get('pmid') or metadata.get('PMID') or metadata.get('pmid') or 'N/A'\n",
        "    pmcid = article.get('PMCID') or article.get('pmcid') or metadata.get('PMCID') or metadata.get('pmcid') or 'N/A'\n",
        "\n",
        "    return f\"\"\"\n",
        "Article {idx}:\n",
        "Title: {metadata.get('title', 'Unknown')}\n",
        "Journal: {journal} | Year: {metadata.get('year', 'N/A')}\n",
        "Type: {metadata.get('paper_type', 'Unknown')}\n",
        "Score: {article.get('score', 0):.1f}\n",
        "Key Concepts Found: {events_str}\n",
        "PMID: {pmid} | PMCID: {pmcid}\n",
        "\n",
        "Full Text:\n",
        "{article.get('content', 'No content available')}...\n",
        "\"\"\"\n",
        "\n",
        "def create_final_analysis_prompt(case_text, disease, events, articles, custom_template):\n",
        "    \"\"\"Create the final analysis prompt with full article contents.\"\"\"\n",
        "\n",
        "    if not articles:\n",
        "        return None\n",
        "\n",
        "    # Format all articles\n",
        "    articles_content_parts = []\n",
        "    for idx, article in enumerate(articles, 1):\n",
        "        articles_content_parts.append(format_article_for_analysis(article, idx))\n",
        "\n",
        "    # Join all articles with separator\n",
        "    articles_content = (\"\\n\" + \"=\"*80 + \"\\n\").join(articles_content_parts)\n",
        "\n",
        "    # Fill in the template\n",
        "    filled_prompt = custom_template.format(\n",
        "        case_description=case_text,\n",
        "        primary_focus=disease,\n",
        "        key_concepts=', '.join(events),\n",
        "        articles_content=articles_content\n",
        "    )\n",
        "\n",
        "    return filled_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "hide_input": true,
        "id": "generate_final_analysis",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "def generate_final_analysis(results, articles_to_analyze, custom_template):\n",
        "    \"\"\"Generate comprehensive final analysis of the literature with visible streaming.\"\"\"\n",
        "\n",
        "    if not articles_to_analyze:\n",
        "        return \"‚ùå No articles available for analysis.\"\n",
        "\n",
        "    print(f\"üîÑ Generating final analysis for {len(articles_to_analyze)} articles...\")\n",
        "\n",
        "    # Create the prompt\n",
        "    prompt = create_final_analysis_prompt(\n",
        "        results['case_text'],\n",
        "        results['disease'],\n",
        "        results['events'],\n",
        "        articles_to_analyze,\n",
        "        custom_template\n",
        "    )\n",
        "\n",
        "    if not prompt:\n",
        "        return \"‚ùå Could not create analysis prompt.\"\n",
        "\n",
        "    # Stream the response with visible tokens\n",
        "    full_response = \"\"\n",
        "\n",
        "    try:\n",
        "        print(\"Streaming tokens: \", end=\"\", flush=True)\n",
        "\n",
        "        for chunk in client.models.generate_content_stream(\n",
        "            model=MODEL_ID,\n",
        "            contents=[prompt],\n",
        "            config=GenerateContentConfig(\n",
        "                temperature=0.3,\n",
        "                max_output_tokens=8192,\n",
        "                thinking_config=types.ThinkingConfig(thinking_budget=THINKING_BUDGET)\n",
        "            )\n",
        "        ):\n",
        "            if chunk.text:\n",
        "                full_response += chunk.text\n",
        "                # Show last 50 characters of the response\n",
        "                display_text = full_response[-50:].replace('\\n', ' ')\n",
        "                print(f\"\\rStreaming tokens: ...{display_text}\", end=\"\", flush=True)\n",
        "\n",
        "        print(\"\\r‚úÖ Analysis complete!                                                           \")\n",
        "        return full_response\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error generating response: {str(e)}\"\n",
        "        print(f\"\\n‚ùå {error_msg}\")\n",
        "        return error_msg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "54562717e2a4"
      },
      "source": [
        "### 11. Interactive Chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "hide_input": true,
        "id": "B3b0C1jfBBKG",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "def medical_qa(results, question):\n",
        "    \"\"\"Ask a question about the medical case analysis with streaming response.\"\"\"\n",
        "    # Build context from top articles\n",
        "    top_articles = results['articles'].head(5)\n",
        "\n",
        "    context = f\"\"\"Medical Case Context:\n",
        "Disease: {results['disease']}\n",
        "Actionable Events: {', '.join(results['events'])}\n",
        "\n",
        "Top Research Articles:\n",
        "\"\"\"\n",
        "\n",
        "    for _, article in top_articles.iterrows():\n",
        "        context += f\"\"\"\n",
        "- PMID {article['PMID']}: {article['title']}\n",
        "  Key findings: {article.get('key_findings', 'N/A')}\n",
        "\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Please provide a detailed, evidence-based response. Cite specific PMIDs when referencing research.\n",
        "\"\"\"\n",
        "\n",
        "    # Create content for the request\n",
        "    contents = [\n",
        "        types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[types.Part(text=prompt)]\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Configure generation settings\n",
        "    generate_content_config = types.GenerateContentConfig(\n",
        "        temperature=0.3,\n",
        "        top_p=0.95,\n",
        "        max_output_tokens=8192,\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            thinking_budget=THINKING_BUDGET,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Stream the response with visible tokens\n",
        "    full_response = \"\"\n",
        "    try:\n",
        "        print(\"Streaming answer: \", end=\"\", flush=True)\n",
        "\n",
        "        for chunk in client.models.generate_content_stream(\n",
        "            model=MODEL_ID,\n",
        "            contents=contents,\n",
        "            config=generate_content_config,\n",
        "        ):\n",
        "            if chunk.text:\n",
        "                full_response += chunk.text\n",
        "                # Show last 50 characters of the response\n",
        "                display_text = full_response[-50:].replace('\\n', ' ')\n",
        "                print(f\"\\rStreaming answer: ...{display_text}\", end=\"\", flush=True)\n",
        "\n",
        "        print(\"\\r‚úÖ Answer complete!                                                           \")\n",
        "        return full_response\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error generating response: {str(e)}\"\n",
        "        print(f\"\\n‚ùå {error_msg}\")\n",
        "        return error_msg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b59783e4f1ce"
      },
      "source": [
        "## Complete Example: Analyzing a Medical Case\n",
        "\n",
        "Let's analyze a sample pediatric leukemia case through the complete pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58edb2bd860f"
      },
      "outputs": [],
      "source": [
        "# Sample case notes\n",
        "SAMPLE_CASE = \"\"\"\n",
        "A 4-year-old male presents with a 3-week history of progressive fatigue, pallor, and easy bruising.\n",
        "Physical examination reveals hepatosplenomegaly and scattered petechiae.\n",
        "\n",
        "Laboratory findings:\n",
        "- WBC: 45,000/ŒºL with 80% blasts\n",
        "- Hemoglobin: 7.2 g/dL\n",
        "- Platelets: 32,000/ŒºL\n",
        "\n",
        "Flow cytometry: CD33+, CD13+, CD117+, CD34+, HLA-DR+, CD19-, CD3-\n",
        "\n",
        "Cytogenetics: 46,XY,t(9;11)(p21.3;q23.3)\n",
        "Molecular: KMT2A-MLLT3 fusion detected, FLT3-ITD positive, NRAS G12D mutation\n",
        "\n",
        "Diagnosis: KMT2A-rearranged acute myeloid leukemia (AML)\n",
        "\"\"\"\n",
        "\n",
        "print(\"üìã Sample Case Notes:\")\n",
        "print(SAMPLE_CASE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87be7f661f14"
      },
      "source": [
        "### Step 1: Run the Analysis Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpuPIhbCBBKH"
      },
      "outputs": [],
      "source": [
        "# Extract Medical Information from Case\n",
        "print(\"üî¨ Extracting medical information from case notes...\")\n",
        "medical_info = extract_medical_info(SAMPLE_CASE)\n",
        "\n",
        "# Display extracted information\n",
        "print(\"\\nüìã Extracted Information:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Disease: {medical_info.get('disease', '')}\")\n",
        "print(f\"\\nActionable Events:\")\n",
        "for i, event in enumerate(medical_info.get('events_list', []), 1):\n",
        "    print(f\"  {i}. {event}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Allow users to modify extracted information if needed\n",
        "# You can edit these values before running the analysis\n",
        "DISEASE = medical_info.get('disease', '')\n",
        "EVENTS = medical_info.get('events_list', [])\n",
        "\n",
        "# Convert events to list if user modified it as string\n",
        "if isinstance(EVENTS, str):\n",
        "    EVENTS = [e.strip() for e in EVENTS.split(',') if e.strip()]\n",
        "\n",
        "# Display final values that will be used\n",
        "print(\"\\n‚úÖ Values to be used for analysis:\")\n",
        "print(f\"Disease: {DISEASE}\")\n",
        "print(f\"Events: {EVENTS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fdad0c3f1f3"
      },
      "outputs": [],
      "source": [
        "# Analysis Configuration Parameters\n",
        "DEFAULT_ARTICLES = 10 # @param {type: \"slider\", min: 1, max: 20, step: 1}\n",
        "MIN_ARTICLES_PER_EVENT = 3 # @param {type: \"slider\", min: 1, max: 10, step: 1}\n",
        "MAX_ARTICLES_TO_SEARCH = 50 # @param {type: \"slider\", min: 10, max: 100, step: 5}\n",
        "\n",
        "# Define your custom scoring criteria\n",
        "# Each criterion should have:\n",
        "# - name: unique identifier (will be used as field name)\n",
        "# - description: what to look for in the article\n",
        "# - type: 'boolean' (true/false), 'numeric' (0-100), 'direct' (direct count), or 'special' (computed)\n",
        "# - weight: points to assign (can be negative for penalties)\n",
        "\n",
        "CUSTOM_CRITERIA = [\n",
        "    # Special criteria (these have custom computation logic)\n",
        "    {\"name\": \"journal_impact\", \"description\": \"High-impact journal (automatic SJR lookup)\", \"type\": \"special\", \"weight\": 25},\n",
        "    {\"name\": \"year_penalty\", \"description\": \"Penalty per year old\", \"type\": \"special\", \"weight\": -5},\n",
        "    {\"name\": \"event_match\", \"description\": \"Points per matching event\", \"type\": \"special\", \"weight\": 15},\n",
        "\n",
        "    # Quality Factors\n",
        "    {\"name\": \"novelty\", \"description\": \"Presents novel/innovative findings or approaches\", \"type\": \"boolean\", \"weight\": 10},\n",
        "\n",
        "    # Relevance Factors\n",
        "    {\"name\": \"disease_match\", \"description\": \"Discusses the specific disease from the case\", \"type\": \"boolean\", \"weight\": 70},\n",
        "    {\"name\": \"pediatric_focus\", \"description\": \"Focuses on pediatric patients\", \"type\": \"boolean\", \"weight\": 50},\n",
        "    {\"name\": \"treatment_shown\", \"description\": \"Shows treatment efficacy or outcomes\", \"type\": \"boolean\", \"weight\": 80},\n",
        "    {\"name\": \"drugs_tested\", \"description\": \"Tests or discusses specific drugs/therapies\", \"type\": \"boolean\", \"weight\": 5},\n",
        "\n",
        "    # Study Types\n",
        "    {\"name\": \"clinical_trial\", \"description\": \"Is a clinical trial\", \"type\": \"boolean\", \"weight\": 50},\n",
        "    {\"name\": \"review_article\", \"description\": \"Is a review article\", \"type\": \"boolean\", \"weight\": -5},\n",
        "    {\"name\": \"case_report\", \"description\": \"Is a case report\", \"type\": \"boolean\", \"weight\": 5},\n",
        "    {\"name\": \"case_series\", \"description\": \"Is a case series or series of case reports\", \"type\": \"boolean\", \"weight\": 10},\n",
        "    {\"name\": \"cell_studies\", \"description\": \"Includes cell/in-vitro studies\", \"type\": \"boolean\", \"weight\": 5},\n",
        "    {\"name\": \"animal_studies\", \"description\": \"Includes animal/mouse model studies\", \"type\": \"boolean\", \"weight\": 10},\n",
        "    {\"name\": \"clinical_study\", \"description\": \"Is a clinical study (observational or interventional)\", \"type\": \"boolean\", \"weight\": 15},\n",
        "    {\"name\": \"clinical_study_on_children\", \"description\": \"Is a clinical study specifically on children\", \"type\": \"boolean\", \"weight\": 20},\n",
        "\n",
        "    # Add your own criteria here! Examples:\n",
        "    # {\"name\": \"biomarker_analysis\", \"description\": \"Analyzes biomarkers or genetic markers\", \"type\": \"boolean\", \"weight\": 15},\n",
        "    # {\"name\": \"survival_data\", \"description\": \"Includes survival or outcome data\", \"type\": \"boolean\", \"weight\": 25},\n",
        "    # {\"name\": \"side_effects\", \"description\": \"Discusses treatment side effects or toxicity\", \"type\": \"boolean\", \"weight\": 10},\n",
        "    # {\"name\": \"sample_size\", \"description\": \"Sample size (0-100 scale where 100 = very large study)\", \"type\": \"numeric\", \"weight\": 0.3},\n",
        "]\n",
        "\n",
        "# Initialize dynamic scoring configuration\n",
        "scoring_config = DynamicScoringConfig(CUSTOM_CRITERIA)\n",
        "\n",
        "# Display current configuration\n",
        "print(\"üìä Analysis Configuration\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Articles per batch: {DEFAULT_ARTICLES}\")\n",
        "print(f\"Minimum articles per event: {MIN_ARTICLES_PER_EVENT}\")\n",
        "print(f\"Maximum articles to search: {MAX_ARTICLES_TO_SEARCH}\")\n",
        "print(\"\\nüìè Scoring Criteria:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Criterion':<30} {'Type':<10} {'Weight':<10} {'Description'}\")\n",
        "print(\"-\" * 60)\n",
        "for criterion in CUSTOM_CRITERIA:\n",
        "    desc = criterion['description'][:40] + '...' if len(criterion['description']) > 40 else criterion['description']\n",
        "    print(f\"{criterion['name']:<30} {criterion['type']:<10} {criterion['weight']:<10} {desc}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Process the case with configurable parameters\n",
        "print(\"\\nüöÄ Starting analysis with custom criteria...\")\n",
        "results = process_medical_case(\n",
        "    SAMPLE_CASE,\n",
        "    default_articles=DEFAULT_ARTICLES,\n",
        "    min_per_event=MIN_ARTICLES_PER_EVENT,\n",
        "    max_articles=MAX_ARTICLES_TO_SEARCH\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2_viz"
      },
      "source": [
        "### Step 2: Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a824202a8f0"
      },
      "outputs": [],
      "source": [
        "# Visualize the results\n",
        "visualize_results(results)\n",
        "\n",
        "# Display top articles\n",
        "display_top_articles(results, n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4_final_analysis"
      },
      "source": [
        "### Step 3: Generate Final Literature Analysis\n",
        "\n",
        "Select articles and customize the analysis prompt to generate a comprehensive literature synthesis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "use_article_selector"
      },
      "outputs": [],
      "source": [
        "# Step 3: Generate Final Literature Analysis\n",
        "\n",
        "# Set your analysis prompt - customize this to get the type of analysis you want\n",
        "ANALYSIS_PROMPT = \"\"\"You are a research analyst synthesizing findings from a comprehensive literature review. Your goal is to provide insights that are valuable for research purposes.\n",
        "\n",
        "RESEARCH CONTEXT:\n",
        "Original Query/Case: {case_description}\n",
        "\n",
        "Primary Focus: {primary_focus}\n",
        "Key Concepts Searched: {key_concepts}\n",
        "\n",
        "ANALYZED ARTICLES:\n",
        "{articles_content}\n",
        "\n",
        "Based on the research context and analyzed articles above, please provide a comprehensive synthesis in markdown format with the following sections:\n",
        "\n",
        "## Literature Analysis: {primary_focus}\n",
        "\n",
        "### 1. Executive Summary\n",
        "Provide a concise overview of the key findings from the literature review, highlighting:\n",
        "- Main themes identified across the literature\n",
        "- Most significant insights relevant to the research query\n",
        "- Overall quality and quantity of available evidence\n",
        "- Key takeaways for researchers in this field\n",
        "\n",
        "### 2. Key Findings by Concept\n",
        "| Concept | Articles Discussing | Key Findings | Evidence Quality |\n",
        "|---------|-------------------|--------------|------------------|\n",
        "[For each key concept searched, summarize what the literature reveals about it. In \"Articles Discussing\", list articles using their PMCID as clickable links, e.g., [PMC7654321](https://pmc.ncbi.nlm.nih.gov/articles/PMC7654321/)]\n",
        "\n",
        "### 3. Methodological Landscape\n",
        "| Research Method | Frequency | Notable Studies | Insights Generated |\n",
        "|-----------------|-----------|-----------------|-------------------|\n",
        "[Map the research methodologies used across the analyzed articles. Reference studies by PMCID]\n",
        "\n",
        "### 4. Temporal Trends\n",
        "| Time Period | Research Focus | Key Developments | Paradigm Shifts |\n",
        "|-------------|----------------|------------------|-----------------|\n",
        "[Analyze how research in this area has evolved over time. Cite articles using PMCID]\n",
        "\n",
        "### 5. Cross-Study Patterns\n",
        "| Pattern | Supporting Evidence | Implications | Confidence Level |\n",
        "|---------|-------------------|--------------|------------------|\n",
        "[Identify patterns that appear across multiple studies. List supporting evidence with PMCID references]\n",
        "\n",
        "### 6. Controversies & Unresolved Questions\n",
        "| Issue | Different Perspectives | Evidence For/Against | Current Consensus |\n",
        "|-------|----------------------|---------------------|-------------------|\n",
        "[Highlight areas of disagreement or ongoing debate in the literature. Cite specific articles by PMCID]\n",
        "\n",
        "### 7. Knowledge Gaps & Future Research\n",
        "| Gap Identified | Why It Matters | Potential Approaches | Expected Impact |\n",
        "|----------------|----------------|---------------------|-----------------|\n",
        "[Map areas where further research is needed based on the analyzed articles]\n",
        "\n",
        "### 8. Practical Applications\n",
        "Based on the synthesized literature, identify:\n",
        "- How these findings can be applied in practice\n",
        "- Recommendations for researchers entering this field\n",
        "- Tools, methods, or frameworks that emerge from the literature\n",
        "- Potential interdisciplinary connections\n",
        "\n",
        "### 9. Quality & Reliability Assessment\n",
        "Evaluate the overall body of literature:\n",
        "- **Study Types**: Distribution of research designs (experimental, observational, reviews, etc.)\n",
        "- **Sample Characteristics**: Common sample sizes, populations studied\n",
        "- **Geographic Distribution**: Where research is being conducted\n",
        "- **Publication Patterns**: Journal quality, publication years, citation patterns\n",
        "- **Methodological Rigor**: Strengths and limitations observed\n",
        "\n",
        "### 10. Synthesis & Conclusions\n",
        "Provide an integrated narrative that:\n",
        "- Connects findings across all analyzed articles\n",
        "- Identifies the strongest evidence and most reliable findings\n",
        "- Suggests how this research area is likely to develop\n",
        "- Offers guidance for stakeholders interested in this topic\n",
        "\n",
        "### 11. Bibliography\n",
        "**Most Relevant Articles** (in order of relevance to the research query):\n",
        "[For each article, format as follows:\n",
        "- Title, Journal (Year). [PMCID: PMCxxxxxx](https://pmc.ncbi.nlm.nih.gov/articles/PMCxxxxxx/) | [PMID: xxxxxxxx](https://pubmed.ncbi.nlm.nih.gov/xxxxxxxx/)]\n",
        "\n",
        "---\n",
        "\n",
        "IMPORTANT NOTES:\n",
        "- When referencing articles throughout the analysis, ALWAYS use their PMCID or PMID identifiers, not generic labels like \"Article 1\"\n",
        "- Format all article references as clickable links: [PMCxxxxxx](https://pmc.ncbi.nlm.nih.gov/articles/PMCxxxxxx/)\n",
        "- Maintain objectivity and clearly distinguish between strong evidence and preliminary findings\n",
        "- Use accessible language while preserving scientific accuracy\n",
        "- All claims must be traceable to specific articles in the analysis\n",
        "- When evidence is conflicting, present all viewpoints fairly\n",
        "- Focus on research insights and knowledge synthesis rather than prescriptive recommendations\n",
        "- Highlight both the strengths and limitations of the current literature\n",
        "\"\"\"\n",
        "\n",
        "# Generate the analysis\n",
        "num_articles = len(results['articles'])\n",
        "\n",
        "if num_articles == 0:\n",
        "    print(\"‚ùå No articles available for analysis.\")\n",
        "else:\n",
        "    # Use all articles from results\n",
        "    all_articles = results['articles'].to_dict('records')\n",
        "\n",
        "    print(f\"üìä Analyzing all {num_articles} retrieved articles...\")\n",
        "\n",
        "    # Generate analysis with streaming indicator\n",
        "    final_analysis = generate_final_analysis(\n",
        "        results,\n",
        "        all_articles,\n",
        "        ANALYSIS_PROMPT\n",
        "    )\n",
        "\n",
        "    # Display formatted result\n",
        "    from IPython.display import display, Markdown\n",
        "    display(Markdown(\"## üìä Final Literature Analysis\"))\n",
        "    display(Markdown(final_analysis))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4_chat"
      },
      "source": [
        "### Step 4: Interactive Medical Consultation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "med_chat"
      },
      "outputs": [],
      "source": [
        "# Step 4: Interactive Medical Consultation\n",
        "\n",
        "# Ask a question about the medical case\n",
        "MEDICAL_QUESTION = \"What is the prognosis for this specific KMT2A rearrangement?\" # @param {type: \"string\"}\n",
        "\n",
        "# Example questions to try:\n",
        "# - \"What is the prognosis for this specific KMT2A rearrangement?\"\n",
        "# - \"What are the key monitoring parameters during treatment?\"\n",
        "# - \"How does the NRAS mutation affect treatment selection?\"\n",
        "# - \"What combination therapies have shown promise for this disease profile?\"\n",
        "\n",
        "if MEDICAL_QUESTION:\n",
        "    from IPython.display import display, Markdown\n",
        "\n",
        "    # Display the question\n",
        "    display(Markdown(f\"### üí¨ Q: {MEDICAL_QUESTION}\"))\n",
        "\n",
        "    # Generate answer with streaming\n",
        "    print(f\"ü§î Analyzing question based on {len(results['articles'])} articles...\")\n",
        "    answer = medical_qa(results, MEDICAL_QUESTION)\n",
        "\n",
        "    # Display formatted answer\n",
        "    display(Markdown(\"### üí° Answer:\"))\n",
        "    display(Markdown(answer))\n",
        "else:\n",
        "    print(\"üí° Enter a question above to get an evidence-based answer from the analyzed literature.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To avoid incurring charges to your Google Cloud account for the resources used in this notebook, follow these steps:\n",
        "\n",
        "1. To avoid unnecessary Google Cloud charges, use the [Google Cloud console](https://console.cloud.google.com/) to delete your project if you do not need it. Learn more in the Google Cloud documentation for [managing and deleting your project](https://cloud.google.com/resource-manager/docs/creating-managing-projects).\n",
        "2. Disable the [Vertex AI API](https://console.cloud.google.com/apis/api/aiplatform.googleapis.com) in the Google Cloud Console."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "e336da7161af",
        "5d7cb7cceb99",
        "u0Z_7bZPBBJv",
        "59c98ab0f5fb",
        "persona_config",
        "a29c93ef3f34",
        "6708e03a1d7b",
        "77f0800f8762",
        "article_analysis",
        "1b308548c68b",
        "fcd767476241",
        "final_analysis_section",
        "54562717e2a4"
      ],
      "name": "PubMed_RAG_Data_Scientist_Example.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
